### 梯度下降思考题

#### 1.关于最小值的问题

梯度下降并不能保证一定能找到全局最小值。它有可能只找到局部最小值，或者停在鞍点（梯度为 0 但不是最小值），甚至可能因为学习率设置不当而发散。能否找到全局最小值，取决于函数的形状、初始点位置、学习率大小以及优化路径的稳定性。

如果函数是凸函数（convex function）：例如 f(x) = x²，这种函数只有一个“碗底”，从任何初始点出发，梯度下降都能最终到达全局最小值。

如果函数是非凸函数（non-convex function）：例如 y = 2x⁵ + x⁴ - 11x³ + 7x² + 2x，这种函数有多个极值点（山谷和山峰）。不同初始点可能会收敛到不同的极小值，因为梯度下降只依赖当前位置的斜率，无法“看见”整个函数的全貌。

​	如在我的梯度下降文件中，若将`np.random.seed(40)` 注释掉，则每次运行的梯度下降图形都不同，甚至还出现收敛到局部最低点的情况；同理，注释掉`np.random.seed(0)`后，每次生成的损失函数图像也有所不同。

##### 影响能否找到最小值的常见因素

1. 初始点位置：起点不同，可能收敛到不同的局部极小值。
2. 学习率：控制每次更新步长，过大容易震荡或发散，过小则收敛太慢。
3. 函数形态：决定是否存在多个极小值。

------

#### 2.梯度下降的主要局限性

1. 局部最小值陷阱：非凸函数常有多个谷底，算法容易卡在某个局部最小值而不是全局最小值，如刚刚提到的不设置种子后出现的问题。
2. 鞍点问题：鞍点是某些方向是谷底、某些方向是山顶的点，梯度为 0，但不是最小值。在高维问题中鞍点比局部最小值更多，算法容易停滞。
3. 学习率敏感：学习率过大会导致震荡甚至发散，过小则使收敛速度极慢。
4. 平坦区（Plateau）问题：当梯度非常小（几乎为 0）时，算法几乎不再更新，导致停滞。
5. 特征尺度敏感：当输入特征的尺度差异很大时（例如 x 与 x⁵），不同方向的梯度变化幅度不同，路径扭曲、收敛慢，需要进行特征标准化。

##### 常见改进方法如：

1. 多次随机初始化：从多个初始点出发训练，提高找到全局最小值的概率。
2. 学习率衰减：让学习率随着迭代次数逐渐减小，前期更新快，后期更稳定。
3. 动量（Momentum）：给参数更新加入“惯性”，减少震荡、帮助越过局部陷阱。
4. 特征归一化：缩放输入特征范围，防止梯度过大或过小。

